Mabel Zhang
30 Aug 2018

Uses BlenSor for rendering depth images of scenes with simulated Kinect sensor



Paths to define:
blender_add_py_paths.py
  Paths of Python files to add to Blender search path
config_paths.py
  Path of data output



==============================================================================
Pipeline summary

1. Render scenes:
$ blensor -b -P scene_generation.py

2. Generate cropped object images from scenes:
$ rosrun depth_scene_rendering postprocess_scenes

3.
[For real run] Run ray-trace occlusion on actual GraspIt contact points:
Collect GraspIt contacts (NOTE: only need to run ONCE for all objects):
$ roslaunch graspit_interface_custom graspit_interface.launch 
$ rosrun grasp_collection grasp_collect.py
Set GEN_RAND_PTS = false, recompile, run:
$ rosrun tactile_occlusion_heatmaps occlusion_test 

[For debugging only] Run ray-trace occlusion test on random points:
Set GEN_RAND_PTS = true, recompile, run (TODO: Make this into a cmd line arg):
$ rosrun tactile_occlusion_heatmaps occlusion_test 


4. (optional)
Inspect data between 3D and 2D:

Visualize 2D visible and occluded contact heat maps:
$ rosrun tactile_occlusion_heatmaps visualize_heatmaps.py [--display]

At the same time, visualize 3D GraspIt saved grasps:
$ rosrun grasp_collection grasp_replay.py


5. Generate .npz files for predictor
$ rosrun tactile_occlusion_heatmaps png_to_npz.py

Data path for predictor is data/npz/, generated by png_to_npz.py.
Define prefix of data files in GQCNN optimizer_constants.py ImageFileTemplates.


==============================================================================
Generate BlenSor scenes

Entry point: scene_generation.py
Core implementation of BlenSor Kinect scanning: scan_kinect.py


To run from terminal:
$ blensor -P scene_generation.py

Headless (add -b):
$ blensor -b -P scene_generation.py



To run from Blender:

In Blender, choose a pane you'd like to switch to console view, press Shift+F4.

From within Blender console, run this to add the Python scripts to Blender
  search path:
  >>> exec(open('/media/master/Data_Ubuntu/courses/research/graspingRepo/visuotactile_grasping/catkin_ws/src/depth_scene_rendering/src/blender_add_py_paths.py').read())

Now Blender knows about all scripts in the src directory.

Import whatever functions you need, from the files in the added directories:
>>> from scan_kinect import scan_kinect

Now you can use the functions in Blender:
>>> scan_kinect ()


==============================================================================
Convert pcd 3D scenes to 2D images with raw depth values

Entry point: postprocess_scenes.cpp

$ rosrun depth_scene_rendering postprocess_scenes


==============================================================================
To pick a range of camera poses to generate random poses within

$ blensor -b -P render_camera_poses.py
Renders from normal Blender RGB camera, at different camera poses in a grid.
Renders are saved to disk.
Used to systematically pick a range of camera poses within which to generate
  random camera poses (in scene_generation.py) for collecting data for training.
  The range is picked such that the object has a chance to appear in any edge
  and corner of the image (not just at the center), for generalization.
Note that the camera renders at 640 x 480, as the Kinect, but training images
  are cropped (e.g. 100 x 100) from the center of the image. So you only need
  to pick ranges so that the object appears within the center 100 x 100, which
  requires a much smaller range than in the entire 640 x 480.

$ python crop_images.py
Crop 100 x 100 in center of image. Must be in center, so that camera intrinsics
  matrix still works.




